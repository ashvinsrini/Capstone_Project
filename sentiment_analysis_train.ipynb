{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np \n",
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "#from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import  LinearSVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pos_reviews=open(\"/Users/ashvinsrinivasan/Desktop/positive.txt\",\"r\", encoding='latin2').read()\n",
    "neg_reviews=open(\"/Users/ashvinsrinivasan/Desktop/negative.txt\",\"r\", encoding='latin2').read()\n",
    "\n",
    "####### Documents have each word entries with its associated category ######\n",
    "documents=[]\n",
    "for ii in pos_reviews.split('\\n'):\n",
    "    documents.append((ii,'pos'))\n",
    "for ii in neg_reviews.split('\\n'):\n",
    "    documents.append((ii,'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract(word_list):\n",
    "    all_word_types=['J','V','N']\n",
    "    ret_list=[]\n",
    "    temp=nltk.pos_tag(word_list)\n",
    "    for w in temp:\n",
    "        if (w[1][0]) in all_word_types:\n",
    "            ret_list.append(w[0].lower())\n",
    "        \n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 67.5 ms, total: 12.9 s\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "####### pos_reviews_words have words  from positive text and likewise for neg_reviews_words   \n",
    "\n",
    "pos_reviews_words=nltk.word_tokenize(pos_reviews)\n",
    "neg_reviews_words=nltk.word_tokenize(neg_reviews)\n",
    "imp_pos_words=extract(pos_reviews_words)\n",
    "imp_neg_words=extract(neg_reviews_words)\n",
    "\n",
    "##### all_words contain the set of words occuring in all documents ########\n",
    "\n",
    "all_words=[]\n",
    "for ii in imp_pos_words:  # put pos_reviews_words to include all words\n",
    "    all_words.append(ii.lower())\n",
    "for ii in imp_neg_words:  # put neg_reviews_words to include all words\n",
    "    all_words.append(ii.lower())    \n",
    "\n",
    "####### all_words_freq contains all words with its associated frequencies #######\n",
    "all_words_freq=nltk.FreqDist(all_words)\n",
    "word_feats=list(all_words_freq.keys())[:5000]\n",
    "\n",
    "''' find_feats function returns true if each word of the 5000 words is present \n",
    "in the document else false. So every features returned is a 5000 vector with \n",
    "true or false entries\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_var=open('word_feats.pickle','wb')\n",
    "pickle.dump(word_feats,temp_var)\n",
    "temp_var.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_feats(document):\n",
    "    words=nltk.word_tokenize(document)\n",
    "    words=extract(words)\n",
    "    features={}\n",
    "    for w in word_feats:\n",
    "         features[w]=w in words\n",
    "    return features\n",
    "\n",
    "\n",
    "### Creating feature list for each document \n",
    "features_list=[(find_feats(review),category) for (review,category) in documents]\n",
    "random.shuffle(features_list)\n",
    "\n",
    "train_set=features_list[:10000]\n",
    "test_set=features_list[10000:]\n",
    "\n",
    "def find_pred(clf,test_set):\n",
    "    y_pred=[clf.classify(test_set[i][0]) for i in range(len(test_set))]\n",
    "    y_true=[test_set[i][1] for i in range(len(test_set))]\n",
    "    y_pred=pd.Series(y_pred).apply(lambda x:0 if x=='neg' else 1)\n",
    "    y_true=pd.Series(y_true).apply(lambda x:0 if x=='neg' else 1)\n",
    "    accuracy=accuracy_score(y_true,y_pred)*100\n",
    "    f_beta=fbeta_score(y_true,y_pred,beta=0.5)\n",
    "    return accuracy,f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_var=open('train_data.pickle','wb')\n",
    "pickle.dump(train_set,temp_var)\n",
    "temp_var.close()\n",
    "\n",
    "temp_var=open('test_data.pickle','wb')\n",
    "pickle.dump(test_set,temp_var)\n",
    "temp_var.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 66.71686746987952\n"
     ]
    }
   ],
   "source": [
    "train_train=train_set[:9000]\n",
    "train_val=train_set[9000:]\n",
    "    \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, test_set))*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_clf=open('OrigNB.pickle','wb')\n",
    "pickle.dump(classifier,save_clf)\n",
    "save_clf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ashvinsrinivasan/Desktop/Machinelearning/Udacity_dir/Advanced/capstone'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy is 66.26506024096386 and f_beta is 0.6750788643533122\n",
      "CPU times: user 34.8 s, sys: 972 ms, total: 35.8 s\n",
      "Wall time: 35.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(train_set)\n",
    "acc,beta=find_pred(LogisticRegression_classifier,test_set)\n",
    "print('Logistic Regression accuracy is {} and f_beta is {}'.format(acc,beta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "save_clf=open('Logistic_reg.pickle','wb')\n",
    "pickle.dump(LogisticRegression_classifier,save_clf)\n",
    "save_clf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimised Linear SVC accuracy is 64.7590361445783 and f_beta is 0.6530717604543108\n",
      "unoptimised Linear SVC accuracy is 63.704819277108435 and f_beta is 0.6441558441558441\n",
      "CPU times: user 5min 41s, sys: 9.44 s, total: 5min 50s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "######## linear SVC Classifier with grid search for different hyper parameters combination ###########\n",
    "\n",
    "linsvc_params={}\n",
    "linsvc_params['C']=[2**-3,2**-1,2**0,2**2,2**4,2**6,2**8,2**10]\n",
    "dict_map={}\n",
    "var=0\n",
    "linsvc_acc=[]\n",
    "linsvc_beta=[]\n",
    "unopt_linSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "for ii in linsvc_params['C']:\n",
    "        linSVC_classifier = SklearnClassifier(LinearSVC(C=ii))\n",
    "        dict_map[var]=[ii] \n",
    "        linSVC_classifier.train(train_train)\n",
    "        acc,beta=find_pred(linSVC_classifier,train_val)\n",
    "        linsvc_acc.append(acc)\n",
    "        linsvc_beta.append(beta)\n",
    "        var+=1\n",
    "best_linsvcparams=dict_map[np.where(linsvc_acc==np.max(linsvc_acc))[0][0]]        \n",
    "        \n",
    "##### Training lin svc classifier with best hyper parameters\n",
    "linsvc_bestclf = SklearnClassifier(LinearSVC(C=best_linsvcparams[0]))\n",
    "\n",
    "linsvc_bestclf.train(train_set)\n",
    "unopt_linSVC_classifier.train(train_set)\n",
    "\n",
    "acc,beta=find_pred(linsvc_bestclf,test_set)\n",
    "unopt_acc,unopt_beta=find_pred(unopt_linSVC_classifier,test_set)\n",
    "\n",
    "print('optimised Linear SVC accuracy is {} and f_beta is {}'.format(acc,beta))\n",
    "print('unoptimised Linear SVC accuracy is {} and f_beta is {}'.format(unopt_acc,unopt_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_clf=open('LinearSVC.pickle','wb')\n",
    "pickle.dump(linsvc_bestclf,save_clf)\n",
    "save_clf.close()\n",
    "save_clf=open('unopt_LinearSVC.pickle','wb')\n",
    "pickle.dump(unopt_linSVC_classifier,save_clf)\n",
    "save_clf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nu SVC accuracy is 66.41566265060241 and f_beta is 0.6663301362948006\n",
      "unoptimised Nu SVC accuracy is 59.48795180722891 and f_beta is 0.6094117647058824\n"
     ]
    }
   ],
   "source": [
    "######## Nu SVC Classifier with grid search for different hyper parameters combination ###########\n",
    "Nusvc_params={}\n",
    "Nusvc_params['nu']=[0.2,0.4,0.6,0.8]\n",
    "Nusvc_params['kernel']=['rbf','poly']\n",
    "dict_map={}\n",
    "var=0\n",
    "Nusvc_acc=[]\n",
    "Nusvc_beta=[]\n",
    "unopt_NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "for ii in Nusvc_params['nu']:\n",
    "    for jj in Nusvc_params['kernel']:\n",
    "        NuSVC_classifier = SklearnClassifier(NuSVC(nu=ii,kernel=jj))\n",
    "        dict_map[var]=[ii,jj] \n",
    "        NuSVC_classifier.train(train_train)\n",
    "        acc,beta=find_pred(NuSVC_classifier,train_val)\n",
    "        Nusvc_acc.append(acc)\n",
    "        Nusvc_beta.append(beta)\n",
    "        var+=1\n",
    "best_nusvcparams=dict_map[np.where(Nusvc_acc==np.max(Nusvc_acc))[0][0]]        \n",
    "        \n",
    "##### Training Nu SVC classifier with best hyper parameters\n",
    "NuSVC_bestclf = SklearnClassifier(NuSVC(nu=best_nusvcparams[0],\n",
    "                                        kernel=best_nusvcparams[1]))\n",
    "NuSVC_bestclf.train(train_set)\n",
    "acc,beta=find_pred(NuSVC_bestclf,test_set)\n",
    "\n",
    "unopt_NuSVC_classifier.train(train_set)\n",
    "unopt_acc,unopt_beta=find_pred(unopt_NuSVC_classifier,test_set)\n",
    "\n",
    "print('Nu SVC accuracy is {} and f_beta is {}'.format(acc,beta))\n",
    "print('unoptimised Nu SVC accuracy is {} and f_beta is {}'.format(unopt_acc,unopt_beta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_clf=open('NuSVC.pickle','wb')\n",
    "pickle.dump(NuSVC_bestclf,save_clf)\n",
    "save_clf.close()\n",
    "save_clf=open('unopt_NuSVC.pickle','wb')\n",
    "pickle.dump(unopt_NuSVC_classifier,save_clf)\n",
    "save_clf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy is 62.65060240963856 and f_beta is 0.6346826586706646\n",
      "unoptimised Random Forest accuracy is 60.69277108433735 and f_beta is 0.6183206106870229\n"
     ]
    }
   ],
   "source": [
    "######## Random Forest Classifier with grid search for different hyper parameters combination ###########\n",
    "rf_params={}\n",
    "rf_params['n_estimators']=[5,10,15]\n",
    "rf_params['min_samples_split']=[2,10,50]\n",
    "rf_params['criterion']=['entropy']\n",
    "dict_map={}\n",
    "var=0\n",
    "rf_acc=[]\n",
    "rf_beta=[]\n",
    "unopt_rf_classifier=SklearnClassifier(RandomForestClassifier())\n",
    "for ii in rf_params['n_estimators']:\n",
    "    for jj in rf_params['min_samples_split']:\n",
    "        for kk in rf_params['criterion']:\n",
    "            rf_classifier=SklearnClassifier(RandomForestClassifier(n_estimators=ii,\n",
    "                                                               min_samples_split=jj,\n",
    "                                                               criterion=kk))\n",
    "            dict_map[var]=[ii,jj,kk] \n",
    "            rf_classifier.train(train_train)\n",
    "            acc,beta=find_pred(rf_classifier,train_val)\n",
    "            rf_acc.append(acc)\n",
    "            rf_beta.append(beta)\n",
    "            var+=1\n",
    "                                                             \n",
    "                                                                                                                                                               \n",
    "best_rfparams=dict_map[np.where(rf_acc==np.max(rf_acc))[0][0]]\n",
    "\n",
    "####### Best random forest classifier ###########\n",
    "rf_bestclf=SklearnClassifier(RandomForestClassifier(n_estimators=best_rfparams[0],\n",
    "                                                       min_samples_split=best_rfparams[1],\n",
    "                                                       criterion=best_rfparams[2]))\n",
    "rf_bestclf.train(train_set)\n",
    "#print(\"rf_classifier accuracy percent:\", (nltk.classify.accuracy(rf_classifier, test_set))*100)\n",
    "acc,beta=find_pred(rf_bestclf,test_set)\n",
    "\n",
    "unopt_rf_classifier.train(train_set)\n",
    "unopt_acc,unopt_beta=find_pred(unopt_rf_classifier,test_set)\n",
    "\n",
    "print('Random Forest accuracy is {} and f_beta is {}'.format(acc,beta))\n",
    "print('unoptimised Random Forest accuracy is {} and f_beta is {}'.format(unopt_acc,unopt_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_clf=open('randomforest.pickle','wb')\n",
    "pickle.dump(rf_bestclf,save_clf)\n",
    "save_clf.close()\n",
    "save_clf=open('unopt_randomforest.pickle','wb')\n",
    "pickle.dump(unopt_rf_classifier,save_clf)\n",
    "save_clf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHyper parameters for each model is optimized based on the accuracy on validation set(train_val), upon which the model with \\nbest hyper parameters is trained on the entire training set(train_set).\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Hyper parameters for each model is optimized based on the accuracy on validation set(train_val), upon which the model with \n",
    "best hyper parameters is trained on the entire training set(train_set).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
